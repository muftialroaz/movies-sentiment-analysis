{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NG26dg5WF_nB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYCl_3g_Zxa4",
        "outputId": "ab606510-b454-4b6b-ac5b-0ce83e664b89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                     text  label\n",
              "0      I grew up (b. 1965) watching and loving the Th...      0\n",
              "1      When I put this movie in my DVD player, and sa...      0\n",
              "2      Why do people who do not know what a particula...      0\n",
              "3      Even though I have great interest in Biblical ...      0\n",
              "4      Im a die hard Dads Army fan and nothing will e...      1\n",
              "...                                                  ...    ...\n",
              "39995  \"Western Union\" is something of a forgotten cl...      1\n",
              "39996  This movie is an incredible piece of work. It ...      1\n",
              "39997  My wife and I watched this movie because we pl...      0\n",
              "39998  When I first watched Flatliners, I was amazed....      1\n",
              "39999  Why would this film be so good, but only gross...      1\n",
              "\n",
              "[40000 rows x 2 columns]>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('movie.csv', sep=',')\n",
        "df.head"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ugEzUaWxapjE"
      },
      "source": [
        "**1. Data Pre-Processing: cleaning\n",
        "text, remove stopwords,\n",
        "stemming, lemmatization.**\n",
        "\n",
        "Tahap ini melibatkan pra-pemrosesan data untuk membersihkan dan mengubah teks mentah menjadi bentuk yang dapat digunakan oleh SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN8fJ94-ai_R",
        "outputId": "b11ea472-ba79-4729-88cd-b6ab4f669a83"
      },
      "outputs": [],
      "source": [
        "## Data cleaning\n",
        "\n",
        "text = df['text']\n",
        "# 1 Lowering case: Mengubah teks menjadi huruf kecil untuk konsistensi dalam analisis\n",
        "text = text.str.lower()\n",
        "# text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu_AlFjQc4B1",
        "outputId": "b8bc190a-8151-4972-dcfa-64cc01677916"
      },
      "outputs": [],
      "source": [
        "# import re\n",
        "\n",
        "# 2 Menghapus karakter khusus kecuali huruf dan angka\n",
        "text = text.str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
        "# text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1caqEODdeHJ",
        "outputId": "2fc3883a-c372-4ad7-e3f1-565c79e817c3"
      },
      "outputs": [],
      "source": [
        "# 3 Tokenisasi Memisahkan teks menjadi unit-unit yang lebih kecil seperti kata-kata atau frasa.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = text.apply(lambda x: word_tokenize(x))\n",
        "# text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPiBo4Mrd3bv",
        "outputId": "7d51d6ac-5c0b-41bf-dc67-295a069493a9"
      },
      "outputs": [],
      "source": [
        "# 4 Menghapus Stopwords Menghapus kata-kata umum yang tidak memberikan informasi penting dalam analisis.\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))  # Menggunakan kamus stop words Bahasa Inggris\n",
        "text = text.apply(lambda x: [word for word in x if word not in stop_words])\n",
        "# text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-9ygmoyemYI",
        "outputId": "643f0ba5-fcaf-4412-9a61-6b35262009f5"
      },
      "outputs": [],
      "source": [
        "# 5 Lematisasi atau Stemming: Mengubah kata-kata ke bentuk dasar mereka untuk mengurangi variasi kata yang sama\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "text = text.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "# text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ho9YWf6g_xn",
        "outputId": "f8c9d39a-aa89-4f13-82b0-78b7c09d82ad"
      },
      "outputs": [],
      "source": [
        "# 6 Menghapus Kata Pendek: Menghapus kata-kata dengan panjang karakter yang terlalu pendek yang cenderung tidak memiliki makna.\n",
        "\n",
        "min_length = 3  # Menentukan panjang minimum kata yang diizinkan\n",
        "text = text.apply(lambda x: [word for word in x if len(word) >= min_length])\n",
        "# text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcKsE66GhOCn",
        "outputId": "ca38ce58-2672-42ae-9297-fa40d89a0189"
      },
      "outputs": [],
      "source": [
        "# 7 Menggabungkan Kembali Teks: Menggabungkan kembali unit-unit teks yang telah dibersihkan menjadi teks yang telah diolah sepenuhnya.\n",
        "text = text.apply(lambda x: ' '.join(x))\n",
        "# text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0cF-46PJhibK"
      },
      "source": [
        "**2. Feature Extraction: Bag-of-Word atau TF-IDF**\n",
        "\n",
        "Pada tahap ini, fitur-fitur atau representasi numerik diperoleh dari teks yang telah dipreproses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqyvuQoSid3V"
      },
      "outputs": [],
      "source": [
        "#Bag-of-Word\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "new_text = text.head(15000)\n",
        "# Inisialisasi CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Memproses teks dan membangun vocabulary\n",
        "X = vectorizer.fit_transform(new_text)\n",
        "\n",
        "# Membuat DataFrame dari hasil Bag-of-Words\n",
        "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print(df_bow)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Inisialisasi TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "new_text = text.head(100)\n",
        "\n",
        "# Memproses teks dan membangun TF-IDF\n",
        "X = vectorizer.fit_transform(new_text)\n",
        "\n",
        "# Membuat DataFrame dari hasil TF-IDF\n",
        "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print(df_tfidf)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cpBuOcs4h0VW"
      },
      "source": [
        "**3. Pembagian Data**\n",
        "\n",
        "Data yang telah dipreproses dan direpresentasikan dalam bentuk fitur perlu dibagi menjadi set pelatihan (training set) dan set pengujian (test set). Set pelatihan digunakan untuk melatih model SVM, sedangkan set pengujian digunakan untuk menguji kinerja model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g1tlfuD4iKMt"
      },
      "source": [
        "**4. Pelatihan Model**\n",
        "\n",
        "Model SVM dilatih menggunakan set pelatihan yang telah dibagi sebelumnya. SVM mempelajari pola dalam data pelatihan untuk memisahkan kelas positif dan negatif. Pada tahap ini, SVM menemukan hyperplane terbaik yang memaksimalkan margin antara kelas-kelas tersebut."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H1VYztQbiO-k"
      },
      "source": [
        "**5. Evaluasi Model**\n",
        "\n",
        "Setelah model dilatih, langkah selanjutnya adalah menguji kinerjanya pada set pengujian yang telah dipisahkan sebelumnya. Beberapa metrik evaluasi yang umum digunakan dalam analisis sentimen adalah akurasi, presisi, recall, dan F1-score. Metrik ini membantu mengukur sejauh mana model dapat mengklasifikasikan sentimen dengan benar."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MpTVIp41iUYs"
      },
      "source": [
        "**6. Prediksi**\n",
        "\n",
        "Setelah model dievaluasi dengan baik, model tersebut dapat digunakan untuk memprediksi sentimen pada data teks baru. Teks baru akan mengalami pra-pemrosesan yang sama seperti pada tahap 1 dan diubah menjadi fitur-fitur menggunakan metode yang sama seperti pada tahap 2. Kemudian, model SVM digunakan untuk mengklasifikasikan sentimen berdasarkan fitur-fitur tersebut"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
